services:
  llama-server:
    image: ghcr.io/ggml-org/llama.cpp:server
    ports:
      - "8080:8080"
    environment:
      - MEM_LOCK=0
    deploy:
      resources:
        limits:
          memory: 1536M
    # Cambiamos --hf por --model-url para evitar el error de comando inv√°lido
    command: 
      - "--model-url"
      - "https://huggingface.co/bartowski/Llama-3.2-1B-Instruct-GGUF/resolve/main/Llama-3.2-1B-Instruct-Q4_K_M.gguf"
      - "--host"
      - "0.0.0.0"
      - "--port"
      - "8080"
    restart: always
